![image-20220305162713906](images/image-20220305162713906.png)



![image-20220305164240603](images/image-20220305164240603.png)

学习率的大小就是步长,每一步的大小

![image-20220305164253632](images/image-20220305164253632.png)



![image-20220305164938219](images/image-20220305164938219.png)

θ1 '10'的左边求导是负值, 减去负值等于加上,会往右边靠近,接近'10'

θ1 '10'的右边求导是正值, 减去正值,会往左边靠近,接近'10'

就可以得到全局最小值的一个近似值



![image-20220305165145522](images/image-20220305165145522.png)



## 公式

![image-20220305171201345](images/image-20220305171201345.png)

右边求偏导↓↓↓

![image-20220305171121098](images/image-20220305171121098.png)



## 非凸函数和凸函数

### 线性回归的代价函数是凸函数

![image-20220305171623997](images/image-20220305171623997.png)

### 凸函数

![image-20220305171712861](images/image-20220305171712861.png)

### 非凸函数

![image-20220305171653247](images/image-20220305171653247.png)

