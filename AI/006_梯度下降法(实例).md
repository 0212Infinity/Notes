```python
import numpy as np
import matplotlib.pyplot as plt

# 加载数据
data = np.genfromtxt('b.csv',delimiter=',')
x_data = data[:,0]
y_data = data[:,1]
# 散点图
plt.scatter(x_data,y_data)
plt.show()
```

```python
# 最小二乘法
def computeError(x_data,y_data,k,b):
    m = len(x_data)
    totalError = 0
    for i in range(0,m):
        totalError += (x_data[i]*k+b - y_data[i])**2
    return totalError / float(m) / 2.0

# 梯度下降 训练
def gradientDescentRunner(x_data,y_data,k,b,lr,epochs):
    m = len(x_data)
    for i in range(0,epochs):
        new_k = 0
        new_b = 0
        for j in range(0,m):
            val = k*x_data[j]+b - y_data[j]
            new_k += val * x_data[j]
            new_b += val
        k = k - (new_k / m * lr)
        b = b - (new_b / m * lr)
    return k,b
```

```python
# 学习率
lr = 0.0001
# 截距
b=0
# 斜率
k=0
# 最大迭代次数
epochs = 50

print('Starting b={} , k={} , error={}'.format(b,k,computeError(x_data,y_data,k,b)))
print('Running...')
k,b = gradientDescentRunner(x_data,y_data,k,b,lr,epochs)
print('After {} iterations b={} , k={} , error={}'.format(epochs,b,k,computeError(x_data,y_data,k,b)))

plt.plot(x_data,y_data,'b.')
plt.plot(x_data,k*x_data+b,'r')
plt.show()
```

